---
title: "HT4"
format: html
editor: visual
---

## Hoja de Trabajo 4: Árboles de decisión

```{r include=FALSE}

library(psych)
library(dplyr)
library(ggplot2)
library(reshape2)
library(psych)
library(corrplot)
library(RColorBrewer)
library(nortest)
library(lmtest)
library(jtools)
library(rpart)
library(rpart.plot)
library(caret)
library(randomForest)

pricesTrain <- read.csv("train.csv")
```

```{r}
set.seed(456)
p <- 0.7
corte <- sample(nrow(pricesTrain), nrow(pricesTrain) * p)
train <- pricesTrain[corte,]
train['YearsBuilt'] = 2023 - train$YearBuilt
train['YearsRem'] = 2023 - train$YearRemodAdd
test <- pricesTrain[-corte,]
test['YearsBuilt'] = 2023 - test$YearBuilt
test['YearsRem'] = 2023 - test$YearRemodAdd
```

### arbol de regresion

```{r multivariable}
str(train)
numericasTrain <- train[, c('MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearsBuilt', 'YearsRem', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'X1stFlrSF', 'X2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'X3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'SalePrice')]
numericasTest <- test[, c('MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearsBuilt', 'YearsRem', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'X1stFlrSF', 'X2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'X3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'SalePrice')]
library(rpart)
arbol1 <- rpart(SalePrice ~ .,data=numericasTrain)
rpart.plot(arbol1)
```

### que tan bueno es

```{r}
ventas <- numericasTest$SalePrice
test2 <- numericasTest[,-81]
a<-predict(arbol1,newdata=test2)
b1<-mean(a-test$GrLivArea)
plot(test$GrLivArea,a,col="green")
par(new=TRUE)
plot(test$GrLivArea,test$SalePrice,col="blue")

```

### Cambiando la profundidad 

```{r}
arbol2 <- rpart(SalePrice ~ .,data=numericasTrain,cp=0.1)
a2<-predict(arbol2,newdata=test2)
b2<-mean(a2-test$GrLivArea)
arbol3 <- rpart(SalePrice ~ .,data=numericasTrain,cp=0.5)
a3<-predict(arbol3,newdata=test2)
b3<-mean(a3-test$GrLivArea)
arbol4 <- rpart(SalePrice ~ .,data=numericasTrain,cp=0.3)
a4<-predict(arbol4,newdata=test2)
b4<-mean(a4-test$GrLivArea)
```

podemos notar que el arbol con un menor error es cuando tiene una profundidad de 0.3

### Nueva Variable respuesta

```{r}
quantile(pricesTrain$SalePrice, na.rm = T, probs = c(0,0.33,0.66,1))
```

Tomando como referencia las divisiones realizadas por los percentiles, clasificaremos como casa Económica aquellas que se encuentren en el rango $\$34900-\$139000$, casa Intermedia $\$139001-\$189893$ y casa Cara entre $\$189894 - \$755000$

#### Dividir los datos

A continuacion se creará el nuevo DataFrame que contendra una nueva columa con la división anterior, de modo que se pueda saber la clasificación de cada casa

```{r dividir}
orderPrice <- pricesTrain[order(pricesTrain$SalePrice),]
orderPrice['Clasificacion']<- list(1:nrow(orderPrice))
orderPrice <- orderPrice %>% select(-c(Id, MoSold, YrSold, GarageYrBlt, Alley, LotShape, LandContour, Condition2, YearBuilt, Exterior2nd, FireplaceQu, GarageQual, SaleType,BsmtFinType2, BsmtFinSF2, BsmtUnfSF, BsmtFullBath, BsmtHalfBath, X3SsnPorch, GarageFinish))
orderPrice <- orderPrice %>% mutate_at (c("MSSubClass","MSZoning", "Utilities", "LotConfig", "Street", "LandSlope", "Neighborhood", "Condition1", "BldgType", "HouseStyle", "OverallQual", "OverallCond", "RoofStyle", "PavedDrive", "RoofMatl", "Exterior1st", "MasVnrType", "ExterQual", "ExterCond","Foundation", "BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "Heating", "HeatingQC", "CentralAir","Electrical", "Functional", "GarageType", "GarageCond", "PoolQC", "Fence", "MiscFeature", "SaleCondition"), as.factor)

orderPrice$Clasificacion[orderPrice$SalePrice <= 139000] <- 'Economica'

orderPrice$Clasificacion[orderPrice$SalePrice > 139000 & orderPrice$SalePrice <= 189893 ] <- 'Intermedia'

orderPrice$Clasificacion[orderPrice$SalePrice > 189893] <- 'Cara'
```


#### Muestreo

A continuación se separarán los datos en datos de prueba y de entrenamiento. Se realizará un muestreo balanceado de modo que hayan suficientes casas de cada tipo tanto en train como en test.

```{r muestreo}
# COnvertir en factores
orderPrice <- orderPrice%>%mutate_at(c("Clasificacion"),as.factor)
set.seed(456)
economicas <- orderPrice[orderPrice$Clasificacion == 'Economica',]
intermedias <- orderPrice[orderPrice$Clasificacion == 'Intermedia',]
caras <- orderPrice[orderPrice$Clasificacion == 'Cara',]

filasCasasE <- sample(nrow(economicas), nrow(economicas)*0.7)
filasCasasI <- sample(nrow(intermedias), nrow(intermedias)*0.7)
filasCasasC <- sample(nrow(caras), nrow(caras)*0.7)

train <- rbind(economicas[filasCasasE,], intermedias[filasCasasI,], caras[filasCasasC,])
test <- rbind(economicas[-filasCasasE,], intermedias[-filasCasasI,], caras[-filasCasasC,])
```

### Árbol de Clasificación

A continuación se realizará un árbol de decisión cuyo propósito será intentar predecir en qué categoría pertenece cada caso basado en los atributos anteriores

```{r arbolClasificacion}

y<- test[,"Clasificacion"]
dataResp <- test
test <- test%>% select(-c("SalePrice", "Clasificacion"))

train <- train %>% select(-c("SalePrice"))

modeloClasificacion <- rpart(Clasificacion~., train, method = "class")
rpart.plot(modeloClasificacion)

ypred <- predict(modeloClasificacion, newdata = test)
ypred <- apply(ypred, 1, function(x) colnames(ypred)[which.max(x)])
ypred <- factor(ypred)


plot(ypred , col="green",density=20,angle=135)
plot(y, col="blue",density=20,angle=45,add=TRUE, beside = TRUE)
legend("bottom",
c("Predicción del modelo","Datos reales"),
fill=c("green","blue"))

confusionMatrix(ypred, y)
```

Podemos ver en la gráfica, que el modelo predijo menor cantidad de casas caras que las que eran, mientras que predijo mayor cantidad de casas económicas e intermedias. Sin embargo la gráfica aunque no nos indica si las casas están bien clasificadas, por lo que no podemos asegurar que la clasificación es eficiente, basado solo en que clasificó casi la cantidad correcta en cada categoría. Utilizando una matriz de confusión, podemos ver que el árbol tuvo una efectividad de 74.94%, teniendo mayor efectividad clasificando las casas caras y la menor efectividad clasificando las intermedias, lo cual tiene sentido, pues, es más efectivo en los extremos. Además, la mayor cantidad de errores fue presentada al ubicar casas de precio intermedio en casas de precio bajo. 

### Árbol de clasificación con validación cruzada

```{r crossValidation}
set.seed(456)
ct <- trainControl(method = 'cv', number = 10, verboseIter = T)
mcv <- train(train[,-34], train$Clasificacion, trControl = ct, method = 'rpart')

y2pred <- predict(mcv, newdata = test)
confusionMatrix(y2pred, y)
```

Este modelo fue peor que el modelo sin validación cruzada, teniendo una eficacia del 59.68%. Se confundió menos al clasificar las caras, pero tuvo muchos más errores al clasificar las de precio intermedio.

### Cambiando profundidades

#### Profundidad 3

```{r profundidad3}
modeloClasificacion3 <- rpart(Clasificacion~., train, method = "class", maxdepth = 3)
rpart.plot(modeloClasificacion3)

ypred3 <- predict(modeloClasificacion, newdata = test)
ypred3 <- apply(ypred3, 1, function(x) colnames(ypred3)[which.max(x)])
ypred3 <- factor(ypred3)

confusionMatrix(ypred3, y)
```

Con una profundidad de 3 el árbol no genera resultados diferentes del modelo estándar.

#### Profundidad 5

```{r profundidad5}
modeloClasificacion5 <- rpart(Clasificacion~., train, method = "class", maxdepth = 5)
rpart.plot(modeloClasificacion5)

ypred5 <- predict(modeloClasificacion, newdata = test)
ypred5 <- apply(ypred5, 1, function(x) colnames(ypred5)[which.max(x)])
ypred5 <- factor(ypred5)

confusionMatrix(ypred5, y)
```

Con una profundidad de 5 el árbol no genera resultados diferentes del modelo estándar.

#### Profundidad 7

```{r profundidad7}
modeloClasificacion7 <- rpart(Clasificacion~., train, method = "class", maxdepth = 7)
rpart.plot(modeloClasificacion7)

ypred7 <- predict(modeloClasificacion, newdata = test)
ypred7 <- apply(ypred7, 1, function(x) colnames(ypred7)[which.max(x)])
ypred7 <- factor(ypred7)

confusionMatrix(ypred7, y)
```

Con una profundidad de 7 el árbol no genera resultados diferentes del modelo estándar.

### Random forest

```{r randomForest}
mrf <- randomForest(Clasificacion~., train, na.action = na.omit)
summary(mrf)

ypredrf <- predict(mrf, newdata = test)
ypredrf <- factor(ypredrf)

confusionMatrix(ypredrf, y)
```

### 

